\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]
  \setbeamertemplate{itemize items}[circle]
  \setbeamertemplate{theorems}[numbered]
  \setbeamercolor*{structure}{bg=white,fg=blue}
  \setbeamerfont{block title}{size=\normalsize}
}

% \newtheorem{proposition}[theorem]{Proposition}
% \theoremstyle{definition}
% \newtheorem{algorithm}[theorem]{Algorithm}
% \newtheorem{idea}[theorem]{Idea}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{aligned-overset}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{csquotes}
% \usepackage{multicol}
% \usepackage{stmaryrd}
\usepackage{tabularx}

% \renewcommand\tabularxcolumn[1]{m{#1}}
% \newcolumntype{R}{>{\raggedleft\arraybackslash}X}=

\def\code#1{\texttt{\frenchspacing#1}}
\def\padding{\vspace{0.5cm}}
\def\spadding{\vspace{0.25cm}}
\def\b{\textcolor{blue}}
\def\r{\textcolor{red}}
\def\g#1{{\usebeamercolor[fg]{block title example}{#1}}}

% fix for \pause in align
\makeatletter
\let\save@measuring@true\measuring@true
\def\measuring@true{%
  \save@measuring@true
  \def\beamer@sortzero##1{\beamer@ifnextcharospec{\beamer@sortzeroread{##1}}{}}%
  \def\beamer@sortzeroread##1<##2>{}%
  \def\beamer@finalnospec{}%
}
\makeatother

\title[DWT revision course]{Discrete Probability Theory \\ revision course}
\author{Jonas HÃ¼botter}
\date{}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
 \tableofcontents[subsectionstyle=hide, subsubsectionstyle=hide]
\end{frame}
\AtBeginSection[]
  {
     \begin{frame}[allowframebreaks]{Plan}
     \tableofcontents[currentsection, sectionstyle=show/hide, hideothersubsections]
     \end{frame}
  }

\section{Counting}

\subsection{Sample spaces and events}
\begin{frame}{Sample spaces and events}
    \begin{definition}
        A \b{sample space} is the set of all possible outcomes of an experiment.
    \end{definition}\pause
    \begin{definition}
        An \b{event} is a subset of the sample space.
    \end{definition}\pause\padding
    Naive definition of probability of an event $A$ in sample space $S$:\pause
    \begin{align*}
        P(A) = \frac{\text{\# favorable outcomes}}{\text{\# possible outcomes}} = \frac{|A|}{|S|}
    \end{align*}\pause
    Assumptions:\pause
    \begin{itemize}
        \item all outcomes equally likely\pause
        \item finite sample space
    \end{itemize}
\end{frame}

\subsection{Counting sets}
\begin{frame}{Counting sets}
    \begin{block}{Multiplication rule}
        Consider $i \in [m]$ experiments with $n_i$ possible outcomes. Then the overall number of possible outcomes is
        \begin{align*}
            \prod_{i=1}^m n_i.
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Sampling table}
        Given $n$ objects, select $k$ objects.
        \begin{block}{}\begin{tabularx}{\textwidth}{X||X|X}
            & order & $\neg$ order \\ \hline\hline
            \onslide<1->{replacement} & \onslide<2->{$n^k$} & \onslide<5->{$n + k - 1 \choose k$}\\
            \onslide<1->{$\neg$ replacement} & \onslide<4->{$\frac{n!}{(n - k)!}$} & \onslide<3->{$n \choose k$}
        \end{tabularx}\end{block}
    \end{block}
\end{frame}

\section{Probability}
\subsection{$\sigma$-algebras}
\begin{frame}{$\sigma$-algebras}
    \begin{definition}
        Given the set $S$. The set $\mathcal{A} \subseteq \mathcal{P}(S)$ is a \b{$\sigma$-algebra} over $S$ if the following properties are satisfied:
        \begin{itemize}\pause
            \item $S \in \mathcal{A}$\pause;
            \item if $A \in \mathcal{A}$, then $\bar{A} \in \mathcal{A}$\pause; and
            \item $\forall n \in \mathbb{N}.\ A_n \in \mathcal{A} \implies \bigcup_{n=1}^{\infty} A_n \in \mathcal{A}$.
        \end{itemize}
    \end{definition}\pause\padding
    \r{Why do we need $\sigma$-algebras?}\pause\par
    To describe events in the context of a probability space.
\end{frame}

\subsection{Probability spaces}
\begin{frame}{Probability spaces}
    \begin{definition}
        Given the set $S$ and the $\sigma$-algebra $\mathcal{A}$ over $S$.\pause\ The function
        \begin{align*}
            P: \mathcal{A} \to [0,1]
        \end{align*}
        is a \b{probability measure} on $\mathcal{A}$ if the \b{Kolmogorov axioms} are satisfied:
        \begin{itemize}\pause
            \item $P(S) = 1$;\pause
            \item $P(\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)$ if $\forall i \neq j.\ A_i \cap A_j = \emptyset$.
        \end{itemize}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        For an event $A \in \mathcal{A}$, $P(A)$ is the \b{probability} of $A$.
    \end{definition}\pause
    \begin{definition}
        A \b{probability space} consists of
        \begin{itemize}
            \item a sample space $S$;
            \item a $\sigma$-algebra $\mathcal{A}$ over $S$; and
            \item a probability measure $P$ on $\mathcal{A}$.
        \end{itemize}
    \end{definition}
\end{frame}

\begin{frame}
    For a probability space the following properties hold:\pause
    \begin{itemize}
        \item $P(\emptyset) = 0$\pause
        \item $P(S) = 1$\pause
        \item $0 \leq P(A) \leq 1$ for all $A \in \mathcal{A}$\pause
        \item $P(\bar{A}) = 1 - P(A)$ for all $A \in \mathcal{A}$\pause
        \item if $A, B \in \mathcal{A}$ and $A \subseteq B$, then $P(A) \leq P(B)$
    \end{itemize}
\end{frame}

\begin{frame}
    Also the \b{principle of inclusion-exclusion} holds:
    \begin{align*}
        P(\bigcup_{i=1}^n A_i) = \sum_{I \in [n], I \neq \emptyset} (-1)^{|I| + 1} \cdot P(\bigcap_{i \in I} A_i).
    \end{align*}\pause\par\padding
    And \b{Boole's inequality} holds:
    \begin{align*}
        P(\bigcup_{i=1}^n A_i) \leq \sum_{i=1}^n P(A_i).
    \end{align*}
\end{frame}

\subsection{Joint and marginal probabilities}
\begin{frame}{Joint and marginal probabilities}
    A \b{marginal probability} is the probability of a single event irrespective of other events.\pause\par\spadding
    A \b{joint probability} is the probability of two or more events occurring simultaneously:
    \begin{align*}
        P(A,B) = P(A \cap B).
    \end{align*}
\end{frame}

\section{Conditional probability}
\subsection{Prior and posterior}
\begin{frame}{Prior and posterior}
    Conditional probability \textit{updates} the probability of an event $A$ given some new information $B$.\pause\par\padding
    $P(A)$ is called the \b{prior} and $P(A|B)$ the \b{posterior} probability.\pause\par\padding
    \begin{align*}
        P(A|B) = \frac{P(A,B)}{P(B)}.
    \end{align*}
    The posterior is the joint probability of the event $A$ and the information $B$ relative to the probability of the information $B$.
\end{frame}

\subsection{Independence}
\begin{frame}{Independence}
    Two events are \b{independent} if the occurrence of one event does not affect the probability of occurrence of the other event.\pause\par\padding
    Two events $A$ and $B$ are independent\par
    $\iff P(A|B) = P(A)$\pause\par
    $\iff P(B|A) = P(B)$\pause\par
    $\iff P(A,B) = P(A) P(B)$.
\end{frame}

\subsection{Conditioning}
\begin{frame}{Conditioning}
    Some properties immediately follow from the definition of conditional probability:\pause
    \begin{itemize}
        \item $P(A,B) = P(B) P(A|B)\pause = P(A) P(B|A)$\par as $A \cap B = B \cap A$\pause
        \item $P(A_1, \dots, A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1, A_2) \cdots P(A_n|A_1, \dots, A_{n-1})$ (\b{multiplication rule})\pause
        \item $P(A|B) = \frac{P(B|A) P(A)}{P(B)}$ (\b{Bayes' rule})\pause
        \item $P(A) = P(A,B) + P(A,\bar{B}) = P(A|B) P(B) + P(A|\bar{B}) P(\bar{B})$ (\b{law of total probability})
    \end{itemize}
\end{frame}

\section{Discrete random variables}
\begin{frame}{Discrete random variables}
    \begin{definition}
        A \b{random variable} $X$ is a function
        \begin{align*}
            X: S \to \mathbb{R}.
        \end{align*}\pause
        A random variable is \b{discrete} if its domain $S$ is finite or countable infinite.
    \end{definition}\pause\padding
    The range of a discrete random variable
    \begin{align*}
        X(S) = \{x \in \mathbb{R}.\ \exists A \in S.\ X(A) = x\}
    \end{align*}
    is also discrete.
\end{frame}

\subsection{Cumulative Distribution Function}
\begin{frame}{Cumulative Distribution Function}
    $X \leq x$ is an event.\pause\par\padding
    \begin{definition}
        The \b{cumulative distribution function} of a random variable $X$ is defined as $F_X(x) = P(X \leq x) \in [0,1]$.
    \end{definition}\pause\par\spadding
    Properties of CDFs:\pause
    \begin{itemize}
        \item monotonically increasing\pause
        \item right-continuous\pause
        \item $F_X(x) \xrightarrow{x \to - \infty} 0$\pause
        \item $F_X(x) \xrightarrow{x \to \infty} 1$
    \end{itemize}\pause\padding
    Therefore, $P(a < X \leq b) = F_X(b) - F_X(a)$.
\end{frame}

\subsection{Probability Mass Function}
\begin{frame}{Probability Mass Function}
    \begin{definition}
        The \b{probability mass function} of a discrete random variable $X$ is defined as $f_X(x) = P(X = x) \in [0,1]$ where
        \begin{align*}
            \sum_{x \in X(S)} f_X(x) = 1.
        \end{align*}
    \end{definition}
\end{frame}

\begin{frame}
    The CDF of $X$ can be obtained from the PDF of $X$ by summing over the PDF
    \begin{align*}
        F_X(x) = \sum_{x' \leq x} f_X(x').
    \end{align*}\pause
    The PMF of $X$ can be obtained from the CDF of $X$ by identifying the \textit{jumps} in the CDF
    \begin{align*}
        f_X(x) = F_X(x) - F_X(prev(x)).
    \end{align*}
\end{frame}

\subsection{Independence}
\begin{frame}{Independence}
    Two random variables are \b{independent} if knowledge about the value of one random variable does not affect the probability distribution of the other random variable.\pause\par\padding
    Two discrete random variables $X$ and $Y$ are independent\par
    $\iff$ the events $X=x$ and $Y=y$ are independent\pause\par
    $\iff$ the events $X \leq x$ and $Y \leq y$ are independent.
\end{frame}

\subsection{Bernoulli}
\begin{frame}{Bernoulli}
    \begin{definition}[$X \sim Bern(p)$]
        A discrete random variable $X$ is \b{Bernoulli} distributed with parameter $p$ when $X(S) = \{0,1\}$ and $P(X=1) = p$.
    \end{definition}\pause
    \begin{exampleblock}{Overview}
        \begin{itemize}
            \item $E(X) = p$
            \item $Var(X) = p (1 - p)$
            \item $G_X(s) = 1 - p + p s$
            \item $M_X(s) = 1 - p + p e^s$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\subsection{Averages}
\begin{frame}{Averages}
    \begin{definition}
        The \b{expected value} $E(X)$ of a random variable $X$ is the arithmetic mean of a large number of a realizations of $X$.\pause
        \begin{align*}
            E(X) &= \sum_{x \in X(S)} x \cdot P(X = x)\pause \\
                 &= \sum_{A \in S} X(A) \cdot P(A).
        \end{align*}
    \end{definition}\pause\par\padding
    For infinite probability spaces \r{absolute convergence} of $E(X)$ is necessary for the existence of $E(X)$.
\end{frame}

\begin{frame}
    Properties of expected values:\pause
    \begin{itemize}
        \item if $\forall A \in S.\ X(A) \leq Y(A)$, then $E(X) \leq E(Y)$ (\b{monotonicity})\pause
        \item $E(a \cdot X + b) = a \cdot E(X) + b$, $E(X + Y) = E(X) + E(Y)$ (\b{linearity})\pause
        \item $E(\prod_{i=1}^n X_i) = \prod_{i=1}^n E(X_i)$ if $X_1, \dots, X_n$ independent (\b{multiplicativity}).
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{definition}
        $E(X^i)$ is called the \b{$i$-th moment} of the random variable $X$ and $E((X - E(X))^i)$ is called the \b{$i$-th central moment} of $X$.
    \end{definition}
\end{frame}

\begin{frame}
    The \b{law of the unconscious statistician (LOTUS)} can be used to find the expected value of transformed random variables.
    \begin{align*}
        E(g(X)) = \sum_{x \in X(S)} g(x) \cdot P(X = x).
    \end{align*}
\end{frame}

\subsection{Indicator variables}
\begin{frame}{Indicator variables}
    \begin{definition}
        Given an event $A$, the random variable $I_A \sim Bern(P(A))$ is the \b{indicator variable} of the event $A$.
    \end{definition}\pause\par\padding
    Properties of indicator variables:\pause
    \begin{itemize}
        \item $E(I_A) = P(A)$ (\b{fundamental bridge})\pause
        \item $E(I_{A_1} \cdots I_{A_n}) = P(A_1 \cap \cdots \cap A_n)$.
    \end{itemize}
\end{frame}

\subsection{Binomial}
\begin{frame}{Binomial}
    \begin{definition}[$X \sim Bin(n,p)$]
        A discrete random variable $X$ has the \b{binomial} distribution with parameters $n$ and $p$ when $X$ models the \#successes in $n$ independent $Bern(p)$ trials.
    \end{definition}\pause
    \begin{align*}
        f_X(k) = {n \choose k} p^k (1 - p)^{n-k}.
    \end{align*}\pause
    \begin{exampleblock}{Overview}
        \begin{itemize}
            \item $E(X) = n p$\pause
            \item $Var(X) = n p (1 - p)$
            \item $G_X(s) = (1 - p + p s)^n$
            \item $M_X(s) = (1 - p + p e^s)^n$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\subsection{Variance}
\begin{frame}{Variance}
    \begin{definition}
        The \b{variance} $Var(X)$ of a random variable $X$ is a measure of the absolute deviation of a random variable from its mean.
        \begin{align*}
            Var(X) &= E((X - E(X))^2)\pause \\
                   &= E(X^2) - E(X)^2.
        \end{align*}\pause
        $SD(X) = \sqrt{Var(X)}$ is the \b{standard deviation} of $X$.
    \end{definition}\pause\par\padding
    Properties of variances:\pause
    \begin{itemize}
        \item $Var(a \cdot X + b) = a^2 \cdot Var(X)$\pause
        \item $Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i)$ if $X_1, \dots, X_n$ independent.
    \end{itemize}
\end{frame}

\subsection{Geometric}
\begin{frame}{Geometric}
    \begin{definition}[$X \sim Geom(p)$]
        A discrete random variable $X$ has the \b{geometric} distribution with parameter $p$ when $X$ models the \#trials leading up to a success in independent $Bern(p)$ trials.
    \end{definition}\pause
    \begin{columns}
        \begin{column}{0.5\textwidth}
           \begin{align*}
                f_X(k) = p (1 - p)^{k - 1}, k \in \mathbb{N}.
            \end{align*}
        \end{column}\pause
        \begin{column}{0.5\textwidth}
            \begin{align*}
                F_X(k) = 1 - (1 - p)^{\lfloor k \rfloor}.
            \end{align*}
        \end{column}
    \end{columns}\pause\par\padding
    \begin{exampleblock}{Overview}
        \begin{itemize}
            \item $E(X) = \frac{1}{p}$\pause
            \item $Var(X) = \frac{1 - p}{p^2}$\pause
            \item $G_X(s) = \frac{p s}{1 - (1 - p) s}$
            % \item $M_X(s) = \frac{p e^s}{1 - (1 - p) e^s}$ if $s < - ln(1 - p)$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \begin{block}{Memorylessness}
        Completing $x$ trials that are all failures does not change the probability of the next $y$ trials to include a success.\pause\par\padding
        This property can be formalized as follows:
        \begin{align*}
            P(X > y + x | X > x) = P(X > y).
        \end{align*}\pause\par\padding
        The geometric distribution is the \r{only} memoryless discrete distribution.
    \end{block}
\end{frame}

\subsection{Poisson}
\begin{frame}{Poisson}
    \begin{definition}[$X \sim Po(\lambda)$]
        A discrete random variable $X$ has the \b{Poisson} distribution with parameter $\lambda$ when $X$ models the \#events in a fixed interval with rate $\lambda$ and with events independently occurring of the time since the last event.
    \end{definition}\pause
    \begin{columns}
        \begin{column}{0.5\textwidth}
           \begin{align*}
                f_X(k) = \frac{e^{- \lambda} \cdot \lambda^k}{k!}, k \in \mathbb{N}_0.
            \end{align*}
        \end{column}\pause
        \begin{column}{0.5\textwidth}
            \begin{align*}
                F_X(k) = e^{- \lambda} \cdot \sum_{i=0}^{\lfloor k \rfloor} \frac{\lambda^i}{i!}.
            \end{align*}
        \end{column}
    \end{columns}\pause\par\padding
    \begin{exampleblock}{Overview}
        \begin{itemize}
            \item $E(X) = \lambda$\pause
            \item $Var(X) = \lambda$\pause
            \item $G_X(s) = exp(\lambda (s - 1))$
            \item $M_X(s) = exp(\lambda (e^s - 1))$
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \begin{block}{Poisson approximation to the Binomial}
        Let $X \sim Bin(n, \lambda / n)$.\pause\par
        Then the distribution of $X$ converges to $Po(\lambda)$ as $n \to \infty$\pause\par
        (i.e. for small $\lambda / n$).
    \end{block}
\end{frame}

\subsection{Probability-generating functions}
\begin{frame}{Probability-generating functions}
    \begin{definition}
       Given a discrete random variable $X$ with $X(S) \subseteq \mathbb{N}_0$ the \b{probability-generating function} is defined as
        \begin{align*}
            G_X(s) &= \sum_{x \in X(S)} s^x \cdot P(X=x)\pause \\
                   &= E(s^X).
        \end{align*}
    \end{definition}\pause\par\padding
    The PGF of a random variable $X$ generates the PMF of $X$:
    \begin{align*}
        P(X = i) = \frac{G_X^{(i)}(0)}{i!}.
    \end{align*}
\end{frame}

\begin{frame}
    Properties of probability-generating functions:\pause
    \begin{itemize}
        \item $E(X) = G_X'(1)$\pause
        \item $Var(X) = G_X''(1) + G_X'(1) - (G_X'(1))^2$\pause
        \item $G_{X + t}(s) = s^t \cdot G_X(s), t \in \mathbb{N}_0$\pause
        \item $G_{X + Y}(s) = G_X(s) \cdot G_Y(s)$ if $X,Y$ independent.
    \end{itemize}
\end{frame}

\subsection{Moment-generating functions}
\begin{frame}{Moment-generating functions}
    \begin{definition}
       Given a random variable $X$ the \b{moment-generating function} is defined as
        \begin{align*}
            M_X(s) &= \sum_{x \in X(S)} e^{s x} \cdot P(X=x)\pause \\
                   &= E(e^{s X})\pause \\
                   &= \sum_{i=0}^{\infty} \frac{E(X^i)}{i!} \cdot s^i.
        \end{align*}
    \end{definition}\pause\par\padding
    The MGF of a random variable $X$ generates the $i$-th moment of $X$:
    \begin{align*}
        E(X^i) = M_X^{(i)}(0).
    \end{align*}
\end{frame}

\begin{frame}
    Properties of moment-generating functions:\pause
    \begin{itemize}
        \item $M_X(s) = G_X(e^s)$ if $X(S) \subseteq \mathbb{N}_0$ \pause
        \item $M_{X + Y}(s) = M_X(s) \cdot M_Y(s)$ if $X,Y$ independent.
    \end{itemize}
\end{frame}

\subsection{Joint distributions}
\begin{frame}{Joint distributions}
    \begin{definition}
        A \b{joint distribution} is the distribution of two or more random variables.
        \begin{align*}
            f_{X,Y}(x,y) = P(X = x, Y = y).
        \end{align*}\pause\par\spadding
        The \b{marginal distribution} of a random variable can be obtained from a joint distribution by summing over all other random variables:
        \begin{align*}
            f_X(x) = \sum_{y \in Y(S)} f_{X,Y}(x,y).
        \end{align*}
    \end{definition}
\end{frame}

\subsection{Conditional distributions}
\begin{frame}{Conditional distributions}
    \begin{definition}
        Given the joint distribution of two random variables $X$ and $Y$ the \b{conditional distribution} of $X$ given $Y$ is the distribution of $X$ when $Y$ is known to be a particular value.
        \begin{align*}
            f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{f_{Y|X}(y|x) \cdot f_X(x)}{f_Y(y)}.
        \end{align*}\pause\par\spadding
        The \b{conditional expectation} of the random variables $X|Y=y$ is the expected value of the distribution $f_{X|Y=y}$:
        \begin{align*}
            E(X|Y=y) = \sum_{x \in X(S)} x \cdot f_{X|Y}(x|y).
        \end{align*}
    \end{definition}
\end{frame}

\subsection{Convolutions}
\begin{frame}{Convolutions}
    \begin{definition}
        Let $X$ and $Y$ be independent and $Z = X + Y$. Then
        \begin{align*}
            f_Z(z) = \sum_{x \in X(S)} f_X(x) \cdot f_Y(z - x).
        \end{align*}\pause
        The derivation of the distribution of a sum of random variables given the marginal distributions is called \b{convolution}.
    \end{definition}
\end{frame}

\subsection{More distributions}
\begin{frame}{More distributions}
    \begin{definition}[$X \sim HypGeom(r,a,b)$]
        A discrete random variable $X$ has the \b{hypergeometric} distribution with parameters $r, a$ and $b$ when $X$ models the \# of drawn objects that have a specified feature in $r$ draws without replacement from $a + b$ objects where $b$ objects have the specified feature.\pause
        \begin{align*}
            f_X(x) = \frac{{b \choose x}{a \choose r - x}}{{a + b \choose r}}.
        \end{align*}\pause
        \begin{exampleblock}{Overview}
            \begin{itemize}
                \item $E(X) = r \cdot \frac{b}{a + b}$
            \end{itemize}
        \end{exampleblock}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}[$Z \sim NegBin(n,p)$]
        A discrete random variable $Z$ has the \b{negative binomial} distribution with parameters $n$ and $p$ when $Z$ models the \# of independent $Bern(p)$ trials before the $n$-th success.\pause
        \begin{align*}
            f_Z(z) = {z - 1 \choose n - 1} p^n (1 - p)^{z - n}.
        \end{align*}\pause
        \begin{example}
            Let $X_1, \dots, X_n \sim Geom(p)$ i.i.d.\par
            Then $Z = X_1 + \cdots + X_n \sim NegBin(n,p)$.
        \end{example}
    \end{definition}
\end{frame}

\subsection{Inequalities}
\begin{frame}{Inequalities}
    \begin{block}{Inequalities vs approximations}
        \textit{Approximations} allow us to model more complex problems but you usually don't know how good the approximation is.\par\pause
        \textit{Inequalities} allow us to prove definite facts (i.e. bounds) about probabilities of certain events.
    \end{block}
\end{frame}

\begin{frame}
    \begin{definition}[Markov]
        Given a random variable $X \geq 0$ and $t > 0$
        \begin{align*}
            P(X \geq t) \leq \frac{E(X)}{t}.
        \end{align*}
    \end{definition}\pause
    \begin{definition}[Chebyshev]
        Given a random variable $X$ and $t > 0$
        \begin{align*}
            P(|X - E(X)| \geq t) \leq \frac{Var(X)}{t^2}.
        \end{align*}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}[Chernoff]
        Let $X_1, \dots, X_n$ be independent, Bernoulli-distributed random variables with $X_i \sim Bern(p_i)$. Then the following inequalities hold for $X = \sum_{i=1}^n X_i$ and $\mu = E(X) = \sum_{i=1}^n p_i$.\pause
        \begin{itemize}
            \item $P(X \geq (1 + \delta) \mu) \leq \left(\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}}\right)^{\mu}$ for all $\delta > 0$;
            \item $P(X \leq (1 - \delta) \mu) \leq \left(\frac{e^{- \delta}}{(1 - \delta)^{1 - \delta}}\right)^{\mu}$ for all $0 < \delta < 1$\pause;
            \item $P(X \geq (1 + \delta) \mu) \leq e^{- \mu \delta^2 / 3}$ for all $0 < \delta \leq 1$;
            \item $P(X \leq (1 - \delta) \mu) \leq e^{- \mu \delta^2 / 2}$ for all $0 < \delta \leq 1$;
            \item $P(|X - \mu| \geq \delta \mu) \leq 2 e^{- \mu \delta^2 / 3}$ for all $0 < \delta \leq 1$;
            \item $P(X \geq (1 + \delta) \mu) \leq \left(\frac{e}{1 + \delta}\right)^{(1 + \delta) \mu}$; and
            \item $P(X \geq t) \leq 2^{-t}$ for all $t \geq 2 e \mu$.
        \end{itemize}
    \end{definition}
\end{frame}

\section{Continuous random variables}
\begin{frame}{Continuous random variables}
    \begin{definition}
        A \b{continuous random variable} $X$ is a function
        \begin{align*}
            X: S \to \mathbb{R}
        \end{align*}
        where $X(S)$ is uncountable.\pause\par\spadding
        The distribution of $X$ is defined by the \b{probability density function} $f_X: \mathbb{R} \to \mathbb{R}_0^+$ with the property
        \begin{align*}
            \int_{- \infty}^{+ \infty} f_X(x)\ dx = 1.
        \end{align*}
    \end{definition}
\end{frame}

\subsection{Continuous probability spaces}
\begin{frame}{Continuous probability spaces}
    \begin{definition}
        An \b{event} is a set $A = \bigcup_k I_k \subseteq \mathbb{R}$ that can be resembled as the union of countably many pairwise disjunct intervals. The probability of $A$ is given as
        \begin{align*}
            P(A) = \int_A f_X(x)\ dx = \sum_k \int_{I_k} f_X(x)\ dx.
        \end{align*}
    \end{definition}\pause\par\padding
    \r{The probability of the event $A = \{x\}, x \in \mathbb{R}$ is always $0$.}
\end{frame}

\begin{frame}
    \begin{block}{Cumulative distribution functions}
        The cumulative distribution function of a continuous random variable $X$ is given as
        \begin{align*}
            F_X(x) &= P(X \leq x)\pause = P(X < x)\pause \\
                   &= \int_{- \infty}^x f_X(t)\ dt.
        \end{align*}\pause\par\spadding
        The PDF of $X$ can be obtained from the CDF of $X$ by finding its derivative with respect to $x$:
        \begin{align*}
            f_X(x) = \frac{dF_X}{dx}.
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Intervals}
        By the fundamental theorem of calculus, the probability of $X$ being in the interval between $a$ and $b$ is given as
        \begin{align*}
            P(a \leq X \leq b) = F_X(b) - F_X(a) = \int_a^b f_X(x)\ dx.
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Expected values}
        The expected value of a continuous random variable $X$ is given as
        \begin{align*}
            E(X) = \int_{- \infty}^{+ \infty} x \cdot f_X(x)\ dx.
        \end{align*}\pause\par\padding
        The law of the unconscious statistician still holds in the continuous case:
        \begin{align*}
            E(g(X)) = \int_{- \infty}^{+ \infty} g(x) \cdot f_X(x)\ dx.
        \end{align*}
    \end{block}
\end{frame}

\subsection{Uniform}
\begin{frame}{Uniform}
    \begin{definition}[$X \sim Unif(a,b)$]
        A continuous random variable $X$ is \b{uniformly} distributed with parameters $a$ and $b$ when $X$ models the outcome of an experiment where all outcomes that lie in the interval $[a,b]$ are equally likely.\pause
        \begin{columns}
            \begin{column}{0.5\textwidth}
               \begin{align*}
                    f_X(x) = \begin{cases}
                        \frac{1}{b - a} & \text{for $x \in [a,b]$} \\
                        0 & \text{otherwise}
                    \end{cases}.
                \end{align*}
            \end{column}\pause
            \begin{column}{0.5\textwidth}
                \begin{align*}
                    F_X(x) = \begin{cases}
                        0 & \text{for $x < a$} \\
                        \frac{x - a}{b - a} & \text{for $x \in [a,b]$} \\
                        1 & \text{for $x > b$}
                    \end{cases}.
                \end{align*}
            \end{column}
        \end{columns}\pause\par\padding
        \begin{exampleblock}{Overview}
            \begin{itemize}
                \item $E(X) = \frac{a + b}{2}$\pause
                \item $Var(X) = \frac{(a - b)^2}{12}$
            \end{itemize}
        \end{exampleblock}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{block}{Universality of the Uniform}
        Let $X \sim F$. Then $F(X) \sim Unif(0,1)$.\pause\par\spadding
        Realizations of a random variable of any distribution $F$ with the inverse CDF $F^{-1}$ can be simulated using realizations of a uniformly distributed random variable $Y$: $F^{-1}(Y) \sim F$.
    \end{block}
\end{frame}

\subsection{Normal (Gaussian)}
\begin{frame}{Normal (Gaussian)}
    \begin{definition}[$X \sim \mathcal{N}(\mu,\sigma^2)$]
        \begin{align*}
            f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \cdot exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right) =: \varphi(x;\mu,\sigma).
        \end{align*}
        \begin{align*}
            F_X(x) =: \Phi(x;\mu,\sigma).
        \end{align*}\pause
        \begin{exampleblock}{Overview}
            \begin{itemize}
                \item $E(X) = \mu$\pause
                \item $Var(X) = \sigma^2$\pause
                \item $M_Z(s) = exp(\mu s + \frac{(\sigma s)^2}{2})$
            \end{itemize}
        \end{exampleblock}
    \end{definition}
\end{frame}

\begin{frame}
    $\mathcal{N}(0,1)$ is the \b{standard normal distribution}.\pause\par\padding
    \begin{block}{Linear transformation}
        Let $X \sim \mathcal{N}(\mu, \sigma^2)$. Then for any $a \in \mathbb{R} \setminus \{0\}$ and $b \in \mathbb{R}$ the random variable
        \begin{align*}
            Y = a X + b
        \end{align*}
        is normally distributed with mean $a \mu + b$ and variance $a^2 \sigma^2$.
    \end{block}\pause\par\padding
    \begin{block}{Standardization}
        Let $X \sim \mathcal{N}(\mu, \sigma^2)$ and $Y = \frac{X - \mu}{\sigma}$. Then $Y \sim \mathcal{N}(0,1)$.\pause\par
        The random variable $Y$ is called \b{standardized}.
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Additivity}
        Let $X_1, \dots, X_n$ independent and normally distributed with parameters $\mu_i, \sigma_i^2$. Then the random variable
        \begin{align*}
            Z = a_1 X_1 + \cdots + a_n X_n
        \end{align*}
        is normally distributed with mean $a_1 \mu_1 + \cdots a_n \mu_n$ and variance $a_1^2 \sigma_1^2 + \cdots + a_n^2 \sigma_n^2$.
    \end{block}\pause\par\padding
    \begin{block}{Normal approximation to the Binomial}
        Let $X \sim Bin(n,p)$ with CDF $F_n(t)$. Then
        \begin{align*}
            F_n(t) \approx \Phi\left(\frac{t - n p}{\sqrt{p (1 - p) n}}\right)
        \end{align*}
        can be used as an approximation if $n p \geq 5$ and $n (1 - p) \geq 5$.
    \end{block}
\end{frame}

\subsection{$\gamma$-quantiles}
\begin{frame}{$\gamma$-quantiles}
    \begin{definition}
        Let $X$ be a continuous random variable with distribution $F_x$. A number $x_{\gamma}$ with
        \begin{align*}
            F_X(x_{\gamma}) = \gamma
        \end{align*}
        is called \b{$\gamma$-quantile} of $X$ or the distribution $F_X$.
    \end{definition}\pause\par\padding
    \begin{definition}
        For the standard normal $z_{\gamma}$ denotes the $\gamma$-quantile.
    \end{definition}
\end{frame}

\subsection{Exponential}
\begin{frame}{Exponential}
    \begin{definition}[$X \sim Exp(\lambda)$]
        A continuous random variable $X$ is \b{exponentially} distributed with parameter $\lambda$ when $X$ models the time between events in a Poisson process.\pause
        \begin{columns}
            \begin{column}{0.5\textwidth}
               \begin{align*}
                    f_X(x) = \lambda e^{- \lambda x}.
                \end{align*}
            \end{column}\pause
            \begin{column}{0.5\textwidth}
                \begin{align*}
                    F_X(x) = 1 - e^{- \lambda x}.
                \end{align*}
            \end{column}
        \end{columns}\pause\par\padding
        \begin{exampleblock}{Overview}
            \begin{itemize}
                \item $E(X) = \frac{1}{\lambda}$\pause
                \item $Var(X) = \frac{1}{\lambda^2}$\pause
                \item $M_X(s) = \frac{\lambda}{\lambda - s}, s < \lambda$
            \end{itemize}
        \end{exampleblock}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{block}{Scaling}
        Let $X \sim Exp(\lambda)$. If $a > 0$, then $Y = a X$ is exponentially distributed with the parameter $\lambda / a$.
    \end{block}\pause\par\padding
    \begin{block}{Memorylessness}
        The exponential distribution is the \r{only} memoryless continuous distribution. Therefore, any continuous random variable $X$ where
        \begin{align*}
            P(X > y + x | X > x) = P(X > y)
        \end{align*}
        holds for all $x,y > 0$ is exponentially distributed.
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Waiting for multiple events}
        Let $X_1, \dots, X_n$ be independent, exponentially distributed random variables with parameters $\lambda_1, \dots, \lambda_n$. Then $X = \min \{X_1, \dots, X_n\}$ is exponentially distributed with parameter $\lambda_1 + \cdots + \lambda_n$.
    \end{block}\pause\par\padding
    \begin{block}{Exponential approximation to the Geometric}
        Let $X_n \sim Geom(\lambda / n)$. The distribution of scaled geometrically distributed random variables $Y_n = \frac{1}{n} X_n$ converges to an exponential distribution with parameter $\lambda$ as $n \to \infty$.
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Poisson process}
        Let $T_1, T_2, \ldots \sim Exp(\lambda)$ i.i.d. that model the time between the $(i - 1)$-st and $i$-th event.\pause\par
        For $t > 0$ we define
        \begin{align*}
            X(t) = \max \{n \in \mathbb{N} \mid T_1 + \cdots + T_n \leq t\}
        \end{align*}
        resembling the number of events that occurred up until time $t$.\pause\par
        Then $X(t)$ is Poisson-distributed with parameter $t \lambda$.
    \end{block}
\end{frame}

\subsection{Joint distributions}
\begin{frame}{Joint distributions}
    \begin{block}{Getting marginals}
        Given a joint distribution $f_{X,Y}$ the marginal distribution $f_X$ can be obtained as follows:
        \begin{align*}
            f_X(x) = \int_{- \infty}^{+ \infty} f_{X,Y}(x,y)\ dy.
        \end{align*}
    \end{block}\pause\par\padding
    \begin{block}{Calculating probabilities}
        Given an event $A \in \mathbb{R}^2$ the probability of $A$ is the area under the probability density function of $X$:
        \begin{align*}
            P(A) = \iint\limits_{A} f_{X,Y}(x,y)\ dx\ dy.
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}
    \begin{block}{Finding PDFs}
        Given a joint CDF $F_{X,Y}$ the joint PDF $f_{X,Y}$ can be obtained as follows:
        \begin{align*}
            f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}}{\partial x \partial y}(x,y).
        \end{align*}
    \end{block}\pause\par\padding
    \begin{block}{Finding CDFs}
        Given a joint PDF $f_{X,Y}$ the joint CDF $F_{X,Y}$ can be obtained as follows:
        \begin{align*}
            F_{X,Y}(x,y) = \int_{- \infty}^y \int_{- \infty}^x f_{X,Y}(u,v)\ du\ dv.
        \end{align*}
    \end{block}
\end{frame}

\subsection{More distributions}
\begin{frame}{More distributions}
    \begin{definition}[$X \sim Lognormal(\mu, \sigma^2)$]
        A continuous random variable $X$ is \b{logarithmically normal} distributed with parameters $\mu$ and $\sigma^2$ when $Y = ln(X) \sim \mathcal{N}(\mu, \sigma^2)$.\pause
        \begin{align*}
            f_X(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \cdot exp\left(-\frac{(ln(x) - \mu)^2}{2 \sigma^2}\right).
        \end{align*}
    \end{definition}
\end{frame}

\section{Inductive Statistics}
\begin{frame}{Inductive Statistics}
    \b{Inductive statistics} aims to use measured quantities to draw conclusions about underlying laws.\pause\par
    To generate data $n$ independent copies of an identical experiment modeled by the random variable $X$ are conducted. A measurement resulting from one of these experiments is called a \b{sample}.\pause\par
    Each sample is represented by a separate random variable $X_i$ called \b{sample variable}.
\end{frame}

\subsection{Estimators}
\begin{frame}{Estimators}
    \begin{definition}
        An \b{estimator} for parameter $\theta$ is a random variable composed of multiple sample variables used to estimate $\theta$.\pause\par\spadding
        The \b{bias} of an estimator $U$ is given as $E(U - \theta)$.\pause\par\spadding
        An estimator $U$ is \b{unbiased} for the parameter $\theta$ if $E(U) = \theta$\par
        (i.e. its bias is zero).
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        The \b{sample mean} $\bar{X}$ is an unbiased estimator for $E(X)$.
        \begin{align*}
            \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i.
        \end{align*}
    \end{definition}\pause\par\padding
    \begin{definition}
        The \b{sample variance} $S^2$ is an unbiased estimator for $Var(X)$.
        \begin{align*}
            S = \sqrt{\frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})^2}.
        \end{align*}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        The \b{mean squared error} is a qualitative measure of an estimator $U$.
        \begin{align*}
            MSE(U) = E((U - \theta)^2).
        \end{align*}\pause
        If $U$ is unbiased, then $MSE(U) = Var(U)$.\pause\par\spadding
        An estimator $A$ is \b{more efficient} than another estimator $B$ if $MSE(A) < MSE(B)$.\par\pause\spadding
        An estimator $U$ is \b{consistent in mean square} if $MSE(U) \xrightarrow{n \to \infty} 0.$
    \end{definition}
\end{frame}

\subsection{Maximum likelihood estimators}
\begin{frame}{Maximum likelihood estimators}
    \b{Maximum Likelihood Construction} is a procedure to construct estimators for parameters of a given distribution.\pause\par\padding
    Given samples $\overrightarrow{X} = (X_1, \dots, X_n)$ find Maximum-Likelihood estimator for $X$ with parameter $\theta$. $\overrightarrow{x} = (x_1, \dots, x_n)$ combines all sample values.
    \begin{enumerate}
        \item construct $L(\overrightarrow{x}; \theta) = \prod_{i=1}^n f_X(x_i; \theta)$
        \item find $\theta$ maximizing $L$
        \item the value for $\theta$ maximizing $L$ is a Maximum-Likelihood estimator for $\theta$
    \end{enumerate}
\end{frame}

\subsection{Law of Large Numbers}
\begin{frame}{Law of Large Numbers}
    The law of large numbers says that the sample mean of i.i.d. sample variables $\bar{X}$ converges to the actual mean $E(X)$ with probability $1$ as the sample size $n$ approaches infinity.\pause
    \begin{align*}
        P(|\bar{X} - E(X)| \geq \delta) \leq \epsilon
    \end{align*}
    for $\delta, \epsilon > 0$ and $n \geq \frac{Var(X)}{\epsilon \delta^2}$.
\end{frame}

\subsection{Central Limit Theorem}
\begin{frame}{Central Limit Theorem}
    The central limit theorem says that the normalized sum of sample values tends towards a standard normal distribution as the sample size approaches infinity even if the original data is not normally distributed.\pause
    \begin{align*}
        \frac{\sum_{i=1}^n X_i - n \mu}{\sigma \sqrt{n}} \xrightarrow{n \to \infty} \mathcal{N}(0,1) \text{ in distribution}
    \end{align*}
    for $X_i$ i.i.d..\pause\par\spadding
    Equivalently:
    \begin{align*}
        \sqrt{n} \left(\frac{\bar{X} - \mu}{\sigma}\right) \xrightarrow{n \to \infty} \mathcal{N}(0,1) \text{ in distribution}.
    \end{align*}
\end{frame}

\begin{frame}
    \begin{block}{De Moivre-Laplace theorem}
        The De Moivre-Laplace theorem is a special case of the central limit theorem and states that the Normal distribution can be used as an approximation for the Binomial distribution.\pause\par\spadding
        Let $X_1, \dots, X_n \sim Bern(p)$ i.i.d. and $H_n = X_1 + \cdots + X_n$. Then
        \begin{align*}
            H_n^* = \frac{H_n - n p}{\sqrt{n p (1 - p)}} \xrightarrow{n \to \infty} \mathcal{N}(0,1) \text{ in distribution}.
        \end{align*}
    \end{block}
\end{frame}

\subsection{Confidence intervals}
\begin{frame}{Confidence intervals}
    Often two estimators are used to approach the estimated quantity from both directions.\pause\par
    The two estimators $U_1$ and $U_2$ are chosen such that
    \begin{align*}
        P(U_1 \leq \theta \leq U_2) \geq 1 - \alpha.
    \end{align*}\pause
    The probability $1 - \alpha$ is called \b{confidence level}.\pause\par\padding
    If for a concrete sample we calculate the estimators $U_1$ and $U_2$ and expect $\theta \in [U_1, U_2]$, then we are only wrong with probability $\alpha$. $[U_1, U_2]$ is a \b{confidence interval}.\pause\par\spadding
    Often a single estimator $U$ is used to define the symmetrical confidence interval $[U - \delta, U + \delta]$.
\end{frame}

\subsection{Hypothesis tests}
\begin{frame}{Hypothesis tests}
    Given sample variables $\overrightarrow{X} = (X_1, \dots, X_n)$ and sample values $\overrightarrow{x} = (x_1, \dots, x_n)$ decide whether to accept or reject a hypothesis.\pause\par\padding
    $K = \{\overrightarrow{x} \in \mathbb{R}^n \mid \overrightarrow{x} \text{ results in rejecting the hypothesis}\}$ is the \b{critical region} (or \b{rejection region}) of a test.\pause\par\padding
    $K$ is constructed based on the concrete values of the \b{test variable} $T$ that is composed of the sample variables.\pause\par\padding
    A test is called \b{one-sided} if $K$ is a half-open interval in $T(S)$ and \b{two-sided} if $K$ is a closed interval in $T(S)$.
\end{frame}

\begin{frame}
    $H_0$ is the hypothesis to be tested, also called \b{null-hypothesis}.\par
    $H_1$ is the \b{alternative}. $H_1$ is \b{trivial} if it is just the negation of $H_0$.\pause\par\padding
    \begin{block}{Errors}
        \begin{itemize}
            \item \b{type 1 error} or \b{$\alpha$-error} or \b{significance level}\par
                $H_0$ holds, but $\overrightarrow{x} \in K$
                \begin{align*}
                    \alpha = \sup_{p \in H_0} P_p(T \in K).
                \end{align*}\pause
            \item \b{type 2 error} or \b{$\beta$-error}\par
                $H_1$ holds, but $\overrightarrow{x} \not\in K$
                \begin{align*}
                    \beta = \sup_{p \in H_1} P_p(T \not\in K).
                \end{align*}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    The \b{quality function} $g$ describes the probability that a test rejects the null-hypothesis.
    \begin{align*}
        g(p) = P_p(T \in K).
    \end{align*}
\end{frame}

\subsection{Statistical tests}
\begin{frame}{Statistical tests}
    \begin{block}{Characteristics}
        Statistical tests can be distinguished by the following characteristics:
        \begin{itemize}
            \item \textbf{Number of involved random variables}\pause\par
                Comparison of two random variables with potentially different distributions (\b{two-sample test}), or examination of a single random variable (\b{one-sample test})?\pause\par
                In case of a two sample test:
                \begin{itemize}
                    \item Independence of involved random variables\par
                        Are \b{independent measurements} (independence) or \b{related measurements} (dependence) taken?\pause
                    \item Relationships between several random variables\par
                        \b{Regression analysis} describes the examination of functional dependencies between random variables, whereas \b{dependency analysis} describes the examination of random variables regarding on independence.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item \textbf{Formulation of the null hypothesis}\pause\par
            Which parameters are examined by the test (e.g. expected value or variance), or is tested for a given distribution?\pause
        \item \textbf{Assumptions}\pause\par
            Which assumptions does the test make regarding independence, distribution, expected value or variance?
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{block}{Important statistical tests}\pause
        \begin{itemize}
            \item Binomial test\pause
            \item $Z$-test\pause
            \item $t$-test\pause
            \item two-sample $t$-test\pause
            \item $\chi^2$-test
        \end{itemize}
    \end{block}
\end{frame}

\section{Markov chains}
\subsection{Stochastic processes}
\begin{frame}{Stochastic processes}
    \begin{definition}
        A \b{stochastic process} is a sequence of random variables $(X_t)_{t \in T}$ that describe the behavior of a system at time $t$.\pause\par\spadding
        If $T = \mathbb{N}_0$, the stochastic process has \b{discrete time}.\pause\ If $T = \mathbb{R}_0^+$, the stochastic process has \b{continuous time}.\par
        If $X_t$ is discrete (i.e. its range is countable), the system is said to have a distinct \b{state} at time $t$.
    \end{definition}
\end{frame}

\subsection{Markov property}
\begin{frame}{Markov property}
    \begin{definition}
        A stochastic process fulfills the \b{Markov property} if the probability distribution of the states at time $t + 1$ solely depends on the probability distribution of states at time $t$, but not on the states at times $< t$.\pause\par\spadding
        This property can be formalized as follows:
        \begin{align*}
            P(X_{t+1} = j | X_t = i_t, \dots, X_0 = i_0) = P(X_{t+1} = j | X_t = i_t)\pause =: p_{i_t j}^t.
        \end{align*}
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        A \b{(finite) Markov chain} (with discrete time) over the state space $S = \{0, \dots, n-1\}$ consists of an infinite sequence of random variables $(X_t)_{t \in \mathbb{N}_0}$ with codomain $S$\pause\ and the \b{initial distribution} $q_0$ with $q_0^T \in \mathbb{R}^n$.\pause\ $q_0$ represents a valid probability mass function (as a row vector) of the random variable $X_0$.\pause\par
        Farther, the Markov property must hold.
    \end{definition}
\end{frame}

\subsection{Representations}
\begin{frame}{Representations}
    \begin{definition}
        If the transition probabilities $p_{ij} = P(X_{t+1} = j | X_t = i)$ are constant over time $t$, the Markov chain is called \b{(time-)homogeneous}.\par\pause\spadding
        In that case the \b{transition matrix} is given as $P = (p_{ij})_{0 \leq i, j < n}$.\par\pause\spadding
        The \b{transition diagram} is a graph consisting of vertices $S$ and weighted edges represented by the adjacency matrix $P$.
    \end{definition}\par\pause\padding
    A concrete instance of the system can be interpreted as a random walk on the transition diagram.
\end{frame}

\subsection{Probabilities}
\begin{frame}{Probabilities}
    The distribution of a Markov chain can be identified iteratively for larger and larger $t$:
    \begin{align*}
        q_{t+1} &= q_t \cdot P\pause \\
        q_t     &= q_0 \cdot P^t\pause \\
        q_{t+k} &= q_t \cdot P^k.
    \end{align*}\pause
    \begin{definition}
        $q_t$ is the \b{state vector} (or \b{distribution}) of the Markov chain at time $t$.
    \end{definition}\par\pause\padding
    The entries of $P^k$ refer to the probability of transitioning from state $i$ to state $j$ in exactly $k$ steps:
    \begin{align*}
        p_{ij}^{(k)} = P(X_{t+k} = j | X_t = i) = (P^k)_{ij}.
    \end{align*}
\end{frame}

\subsection{Hitting times}
\begin{frame}{Hitting times}
    \begin{definition}
        The \b{hitting time} of state $j$ from state $i$ is modeled by the following random variable:
        \begin{align*}
            T_{ij} = \min \{n \geq 1 \mid X_n = j \text{ given } X_0 = i\}.
        \end{align*}\pause
        The \b{expected hitting time} is given as
        \begin{align*}
            h_{ij} &= E(T_{ij})\pause \\
                   &= 1 + \sum_{k \neq j} p_{ik} h_{kj}.
        \end{align*}
    \end{definition}
\end{frame}

\begin{frame}
    The probability of reaching state $j$ from state $i$ in arbitrarily many steps is called \b{arrival probability} $f_{ij}$:
    \begin{align*}
        f_{ij} &= P(T_{ij} < \infty)\pause \\
               &= p_{ij} + \sum_{k \neq j} p_{ik} f_{kj}.
    \end{align*}\pause
    \begin{definition}
        The random variable $T_i = T_{ii}$ refers to the \b{recurrence time} of state $i$ to state $i$.\pause\par\spadding
        The \b{expected recurrence time} $h_i = h_{ii}$ and the \b{recurrence probability} $f_i = f_{ii}$ are defined analogously to the expected hitting time and the arrival probability.
    \end{definition}
\end{frame}

\subsection{Stationary distribution}
\begin{frame}{Stationary distribution}
    \begin{definition}
        A state vector $\pi$ with $\pi = \pi \cdot P$ is a \b{stationary distribution} of a Markov chain.
    \end{definition}\par\pause\padding
    A Markov chain does not necessarily converge to a stationary distribution. Convergence depends on the properties of the Markov chain itself and its initial distribution.
\end{frame}

\subsection{Interlude: Diagonalization}
\begin{frame}{Interlude: Diagonalization}
    For eigenvectors $x_i$ and related eigenvalues $\lambda_i$ of a matrix $A$, $A \cdot x_i = \lambda_i \cdot x_i$ holds.\pause\par\spadding
    Then for a square matrix $A$ with eigenvectors $x_1, \dots, x_n$ and related eigenvalues $\lambda_1, \dots, \lambda_n$\pause, it holds that
    \begin{align*}
        A \cdot \begin{bmatrix}
            x_1 & \cdots & x_n
        \end{bmatrix}\pause &= \begin{bmatrix}
            \lambda_1 x_1 & \cdots & \lambda_n x_n
        \end{bmatrix}\pause \\
        &= \begin{bmatrix}
            x_1 & \cdots & x_n
        \end{bmatrix} \cdot \begin{bmatrix}
            \lambda_1 & 0 & 0 \\
            0 & \ddots & 0 \\
            0 & 0 & \lambda_n
        \end{bmatrix}.
    \end{align*}\pause
    Let $V$ be the matrix consisting of the eigenvectors of $A$ as column vectors and let $\Lambda$ be the diagonal matrix consisting of the eigenvalues of $A$.\pause\par\spadding
    Then $V^{-1} \cdot A \cdot V = \Lambda$ is called \b{diagonalization} of $A$.\pause\par
    Conversely, $A = V \cdot \Lambda \cdot V^{-1}$ holds.
\end{frame}

\subsection{Convergence}
\begin{frame}{Convergence}
    From the diagonalization of the transition matrix it follows that
    \begin{align*}
                                     P^t &= V \cdot \Lambda^t \cdot V^{-1}.
    \end{align*}\pause
    This can be used to describe the behavior of a Markov chain for $t \to \infty$:
    \begin{align*}
        &\lim_{t \to \infty} q_t\pause = \lim_{t \to \infty} q_0 \cdot P^t.\pause \\
        &\lim_{t \to \infty} P(X_t = j \mid X_0 = i)\pause = \lim_{t \to \infty} P^t(i,j).
    \end{align*}
\end{frame}

\subsection{Properties}
\begin{frame}{Properties}
    Certain properties of Markov chains allow us to draw conclusions about its stationary distributions.\pause\par\padding
    \begin{definition}
        A state $i$ is \b{absorbing} if $p_{ii} = 1$, that is its vertex in the transition diagram has no outgoing edges.\pause\par\spadding
        A state $i$ is \b{recurrent} if $f_{i} = 1$, that is with probability $1$ the Markov chain returns to state $i$.\pause\par
        if conversely $f_{i} < 1$, the state $i$ is \b{transient}.
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        A Markov chain is \b{irreducible} if every state is reachable from every other state with a positive probability if the Markov chain is run for enough steps.\pause\ Formally:
        \begin{align*}
            \forall i, j \in S.\ \exists n \in \mathbb{N}.\ p_{ij}^{(n)} > 0.
        \end{align*}\pause
        A finite Markov chain is irreducible if and only if its transition diagram is strongly connected.\pause\par\spadding
        If a finite Markov chain is irreducible\pause
        \begin{itemize}
            \item $f_{ij} = 1, \forall i, j \in S$\pause;
            \item $h_{ij} \text{ exists}, \forall i, j \in S$\pause; and
            \item there exists a unique stationary distribution $\pi$ with $\pi(j) = \frac{1}{h_{j}}, \forall j \in S$.
        \end{itemize}\pause
        \r{The Markov chain does not necessarily converge to the stationary distribution (periodicity!).}
    \end{definition}
\end{frame}

\begin{frame}
    We now want to examine the periodicity of states.\pause
    \begin{definition}
        For a state $i$ define
        \begin{align*}
            T(i) = \{n \geq 1 \mid P^n(i,i) > 0\}.
        \end{align*}\pause
        Then the \b{period} of state $i$ is defined as $d_i = gcd(T(i))$.\pause\par\padding
        If a Markov chain is irreducible, all of its states share the same period. This period is then referred to as the period of the Markov chain.
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        A state $i$ is \b{aperiodic} if $d_i = 1$\pause, or equivalently, if $\exists n_0 \in \mathbb{N}.\ \forall n \geq n_0.\ p_{ii}^{(n)} > 0$.\pause\par\spadding
        Therefore a state $i$ is aperiodic if and only if the transition diagram has a closed path from $i$ to $i$ with length $n$ for all $n \in \mathbb{N}$ greater some $n_0 \in \mathbb{N}$.\pause\par\spadding
        That is state $i$ is surely aperiodic if in the transition diagram
        \begin{itemize}
            \item it has a loop ($p_{ii} > 0$)\pause\ or
            \item it is on at least two closed paths $P_1$ and $P_2$ whose lengths are coprime.
        \end{itemize}\pause\par\padding
        A Markov chain is \b{aperiodic} if all its states are aperiodic.
    \end{definition}
\end{frame}

\begin{frame}
    \begin{definition}
        An ireducible and aperiodic Markov chain is called \b{ergodic}.
    \end{definition}\pause\par\padding
    For every finite ergodic Markov chain it holds independently of its initial distribution $q_0$ that
    \begin{align*}
        \lim_{t \to \infty} q_t = \pi
    \end{align*}
    where $\pi$ refers to its unique stationary distribution.
\end{frame}

\begin{frame}
    \begin{definition}
        A square matrix $A$ is called \b{stochastic} if all its rows sum to one.\pause\par
        Every transition matrix $P$ is stochastic.\pause\par\spadding
        Additionally, $A$ is called \b{doubly stochastic} if also all its columns sum to one.
    \end{definition}\pause\par\padding
    For every finite ergodic Markov chain whose transition matrix is doubly stochastic its unique stationary distribution assigns the same probability to each state:
    \begin{align*}
        \pi \equiv \frac{1}{|S|}.
    \end{align*}
\end{frame}

\end{document}
